{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cras-lab/NLP/blob/main/Text_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face 라이브러리를 이용하여, BART 모델로 문장을 요약해보는 간단한 예시\n",
        "먼저 트랜스포머 라이브러리를 설치한다."
      ],
      "metadata": {
        "id": "lCozl2_oP7kN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvO-Hg3JP4Sx"
      },
      "outputs": [],
      "source": [
        "pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BART( Bidirectional and Auto-Regressive Transformer)는 OpenAI에서 개발한 자연어처리 모델 중 하나로, Transformer 아키텍처를 기반으로 한 seq2seq(Sequence to Sequence) 모델이다.\n",
        "\n",
        "BART 모델은 기본적으로 인코더-디코더 구조를 갖추고 있으며, 인코더에서는 입력 문장을 벡터로 인코딩하고, 디코더에서는 인코딩된 벡터를 바탕으로 출력 문장을 생성한다. BART 모델은 Transformer의 인코더와 디코더 블록 모두에 대해 양방향(bidirectional) 및 자기회귀(auto-regressive) 모델링을 수행하므로, 입력 문장의 양 끝단에 있는 단어들이 문맥적으로 중요하다는 정보를 활용할 수 있다.\n",
        "\n",
        "BART 모델은 기본적으로 대용량 텍스트 데이터를 학습하여 생성 모델링(generative modeling) 작업에 사용된다. 특히, 기계 번역, 요약, 질의응답 등 다양한 자연어 처리 태스크에 적용되며, 최근에는 원하는 스타일이 반영된 텍스트 생성, 즉 스타일 전이(style transfer) 작업에도 사용된다.\n",
        "\n",
        "BART 모델은 다양한 크기의 모델로 제공되며, 더 큰 모델일수록 높은 성능을 보인다. 최근에는 BART 모델의 변형 모델로 T5 모델이 나오면서 인기를 얻고 있다.\n",
        "\n",
        "pipeline을 임포트하고 BART를 기본 모델로 설정 한다."
      ],
      "metadata": {
        "id": "3CF3D7G2QHzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")"
      ],
      "metadata": {
        "id": "9B4PvYYqQPbL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 문장을 주면 그 문장의 요약을 생성해 낸다. 문장을 요약해 보자."
      ],
      "metadata": {
        "id": "dMesR77JQTWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ARTICLE = \"\"\"\n",
        "The best way to learn anything is by doing. That is why we highly encourage you to\n",
        "experiment with the code samples provided (the code can be found in the accompanying\n",
        "GitHub repository), apply the techniques to different datasets, and explore possible\n",
        "extensions.\"\"\"\n",
        "\n",
        "print(summarizer(ARTICLE, max_length=100, min_length=30, do_sample=False))"
      ],
      "metadata": {
        "id": "EbakNwJIQTDB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}